{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "# model_path = \"/proj/vondrick3/bigmodels/llama2_chat/converted_weights_llama_chat70b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, device_map=\"auto\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get latent embeddings in regular forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "[INST] What is highest mountain in the world? [/INST]\n",
    "'''\n",
    "\n",
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "outputs = model.forward_interpret(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "prompt_len = model_inputs['input_ids'].shape[-1]\n",
    "prompt_tokenized = tokenizer.convert_ids_to_tokens(model_inputs['input_ids'][0])\n",
    "prompt_tokenids = model_inputs['input_ids'][0]\n",
    "\n",
    "[(idx, tokenizer.decode([i])) for idx, i in enumerate(prompt_tokenids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify intepretation prompt and replacing locations. Here, _ _ _ _ _ will be replaced. If you are using other interpretation prompts, you can check index of each token to decide replacing locations with\n",
    "\n",
    "    [(idx, tokenizer.decode([i])) for idx, i in enumerate(repeat_prompt_tokenids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_prompts = \"[INST] _ _ _ _ _ [/INST]\\nSure, I'll summerize your message:\"\n",
    "start_insert = 4\n",
    "end_insert = 9\n",
    "\n",
    "model_inputs = tokenizer([repeat_prompt for i in range(len(batch_insert_infos))], return_tensors=\"pt\").to(\"cuda:0\")\n",
    "repeat_prompt_n_tokens = model_inputs['input_ids'].shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up `insert_info`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_insert_infos = []\n",
    "for retrieve_layer in [0, 1, 5, 10,15,20,25,30,35,40,45,50,55,60,65,70,75,79, 80]:\n",
    "    for retrieve_token in range(prompt_len):\n",
    "        insert_info = {}\n",
    "        insert_info['replacing_mode'] = 'normalized'\n",
    "        insert_info['overlay_strength'] = 1\n",
    "        insert_info['retrieve_layer'] = retrieve_layer\n",
    "        insert_info['retrieve_token'] = retrieve_token\n",
    "        for idx, layer in enumerate(model.model.layers):\n",
    "            insert_locations = [i for i in range(start_insert, end_insert)]\n",
    "            insert_info[idx] = (insert_locations, outputs['hidden_states'][retrieve_layer][0][retrieve_token].repeat(1,len(insert_locations), 1))\n",
    "        all_insert_infos.append(insert_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get interpretations by batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_start_idx in range(0,len(all_insert_infos),bs):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        batch_insert_infos = all_insert_infos[batch_start_idx:min(batch_start_idx+bs, len(all_insert_infos))]\n",
    "\n",
    "        model_inputs = tokenizer([repeat_prompt for i in range(len(batch_insert_infos))], return_tensors=\"pt\").to(\"cuda:0\")\n",
    "        repeat_prompt_n_tokens = model_inputs['input_ids'].shape[-1]\n",
    "\n",
    "        output = model.generate_interpret(**model_inputs, max_new_tokens=30, insert_info=batch_insert_infos, pad_token_id=tokenizer.eos_token_id, output_attentions = False)\n",
    "\n",
    "        cropped_interpretation_tokens = output[:,repeat_prompt_n_tokens:]\n",
    "        batch_interpretations = tokenizer.batch_decode(cropped_interpretation_tokens, skip_special_tokens=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another example to also get interpretation logits and relevancy score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_start_idx in range(0,len(all_insert_infos),bs):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        batch_insert_infos = all_insert_infos[batch_start_idx:min(batch_start_idx+bs, len(all_insert_infos))]\n",
    "\n",
    "        model_inputs = tokenizer([repeat_prompt for i in range(len(batch_insert_infos))], return_tensors=\"pt\").to(\"cuda:0\")\n",
    "        repeat_prompt_n_tokens = model_inputs['input_ids'].shape[-1]\n",
    "\n",
    "        output = model.generate_interpret(**model_inputs, max_new_tokens=30, insert_info=batch_insert_infos, pad_token_id=tokenizer.eos_token_id, output_attentions = False)\n",
    "\n",
    "        cropped_interpretation_tokens = output[:,repeat_prompt_n_tokens:]\n",
    "        batch_interpretations = tokenizer.batch_decode(cropped_interpretation_tokens, skip_special_tokens=True)\n",
    "        \n",
    "\n",
    "        \n",
    "        original_attribution_outputs = model.forward_interpret(\n",
    "                                                    output[...,:-1],\n",
    "                                                    return_dict=True,\n",
    "                                                    output_attentions=True,\n",
    "                                                    output_hidden_states=True,\n",
    "                                                    insert_info=batch_insert_infos,\n",
    "                                                )\n",
    "\n",
    "        original_final_logits = model.lm_head(original_attribution_outputs.hidden_states[-1])\n",
    "        original_final_logits = torch.softmax(original_final_logits, dim=-1)\n",
    "\n",
    "        corrupted_attribution_outputs = model.forward_interpret(\n",
    "                        output[...,:-1],\n",
    "                        return_dict=True,\n",
    "                        output_attentions=True,\n",
    "                        output_hidden_states=True,\n",
    "                    )\n",
    "\n",
    "        corrupted_final_logits = model.lm_head(corrupted_attribution_outputs.hidden_states[-1])\n",
    "        corrupted_final_logits = torch.softmax(corrupted_final_logits, dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        diff = (original_final_logits - corrupted_final_logits).abs() \n",
    "\n",
    "        indices = output[:,1:].detach().cpu().long()\n",
    "        indices = indices.unsqueeze(-1)\n",
    "        selected_diff = torch.gather(diff.detach().cpu(), 2, indices).squeeze(-1)\n",
    "\n",
    "        original_probs = torch.gather(original_final_logits.detach().cpu(), 2, indices).squeeze(-1)\n",
    "        cropped_original_probs = original_probs[:,repeat_prompt_n_tokens-1:]\n",
    "\n",
    "        #interpretation logits\n",
    "        interpretation_probs = cropped_original_probs#[:,:10].prod(dim=-1)\n",
    "\n",
    "        #relevancy score\n",
    "        cropped_selected_diff = selected_diff[:,repeat_prompt_n_tokens-1:]\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
